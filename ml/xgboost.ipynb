{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bd19c368",
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import optuna\n",
        "from pathlib import Path\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb9bcd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "gamesDF = pd.read_csv('./datasets/training_dataset.csv')\n",
        "gamesDF = gamesDF.drop(columns=['HOME_L10_LOSSES', 'AWAY_L10_LOSSES', 'PERIOD', 'POINT_DIFF'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8238324d",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = gamesDF[['SECONDS_REMAINING','HOME_SCORE','AWAY_SCORE','HOME_WINS', 'HOME_LOSSES', 'AWAY_WINS', 'AWAY_LOSSES', 'HOME_L10_WINS', 'AWAY_L10_WINS']] \n",
        "y = gamesDF['HOME_WIN']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fff9bef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2026-02-11 12:04:27,452]\u001b[0m A new study created in memory with name: no-name-a6ce9972-7293-45ec-af2e-5a171832a331\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:05:08,312]\u001b[0m Trial 0 finished with value: 0.474746821387863 and parameters: {'n_estimators': 687, 'max_depth': 2, 'learning_rate': 0.02851806938879716, 'subsample': 0.9977370244294912, 'colsample_bytree': 0.8053553948348537, 'reg_lambda': 3.972943439961816, 'reg_alpha': 2.759495093837856, 'min_child_weight': 242, 'gamma': 2.456355360571375}. Best is trial 0 with value: 0.474746821387863.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:05:56,033]\u001b[0m Trial 1 finished with value: 0.4596959175914084 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.02026056101631898, 'subsample': 0.9581586458345048, 'colsample_bytree': 0.9497845015123165, 'reg_lambda': 0.170626814930273, 'reg_alpha': 0.012248432039889645, 'min_child_weight': 280, 'gamma': 0.6779560437997724}. Best is trial 1 with value: 0.4596959175914084.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:06:25,432]\u001b[0m Trial 2 finished with value: 0.45150511719335373 and parameters: {'n_estimators': 591, 'max_depth': 3, 'learning_rate': 0.031234194792817114, 'subsample': 0.8137360918581884, 'colsample_bytree': 0.8068166511591076, 'reg_lambda': 0.01334650166716286, 'reg_alpha': 0.34294108577985755, 'min_child_weight': 259, 'gamma': 4.566017505314827}. Best is trial 2 with value: 0.45150511719335373.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:06:57,077]\u001b[0m Trial 3 finished with value: 0.46948575775730594 and parameters: {'n_estimators': 782, 'max_depth': 2, 'learning_rate': 0.03781791040368314, 'subsample': 0.6843444708849024, 'colsample_bytree': 0.6211540524254967, 'reg_lambda': 0.185582654448078, 'reg_alpha': 0.08014387015911216, 'min_child_weight': 256, 'gamma': 3.8206911560792625}. Best is trial 2 with value: 0.45150511719335373.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:07:26,454]\u001b[0m Trial 4 finished with value: 0.4732216398661883 and parameters: {'n_estimators': 571, 'max_depth': 2, 'learning_rate': 0.04714543612988872, 'subsample': 0.8013644104311671, 'colsample_bytree': 0.6296086069722332, 'reg_lambda': 0.013374093331836591, 'reg_alpha': 0.5132759089976099, 'min_child_weight': 235, 'gamma': 4.666388883398598}. Best is trial 2 with value: 0.45150511719335373.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:07:47,179]\u001b[0m Trial 5 finished with value: 0.49765882213624785 and parameters: {'n_estimators': 392, 'max_depth': 2, 'learning_rate': 0.04004189846837058, 'subsample': 0.8360098860134155, 'colsample_bytree': 0.6286654759580592, 'reg_lambda': 0.8686802830997963, 'reg_alpha': 0.01183754507861718, 'min_child_weight': 233, 'gamma': 2.546850845523797}. Best is trial 2 with value: 0.45150511719335373.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:08:32,711]\u001b[0m Trial 6 finished with value: 0.44688468069351733 and parameters: {'n_estimators': 738, 'max_depth': 3, 'learning_rate': 0.025763372805292215, 'subsample': 0.8295513305547999, 'colsample_bytree': 0.9074207646003157, 'reg_lambda': 0.13632894097953385, 'reg_alpha': 0.01955830834955129, 'min_child_weight': 296, 'gamma': 0.6464059086886818}. Best is trial 6 with value: 0.44688468069351733.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:09:18,400]\u001b[0m Trial 7 finished with value: 0.45689526145911946 and parameters: {'n_estimators': 752, 'max_depth': 2, 'learning_rate': 0.04903801263307454, 'subsample': 0.8199543092550099, 'colsample_bytree': 0.7627308907303593, 'reg_lambda': 0.2415712184418281, 'reg_alpha': 0.15811890505199275, 'min_child_weight': 300, 'gamma': 4.393432789309488}. Best is trial 6 with value: 0.44688468069351733.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:10:02,882]\u001b[0m Trial 8 finished with value: 0.4616025812340705 and parameters: {'n_estimators': 715, 'max_depth': 3, 'learning_rate': 0.025239764940395634, 'subsample': 0.7904140509110662, 'colsample_bytree': 0.6423136514807586, 'reg_lambda': 0.1651493699863997, 'reg_alpha': 0.06492237696456504, 'min_child_weight': 218, 'gamma': 1.3734583794765443}. Best is trial 6 with value: 0.44688468069351733.\u001b[0m\n",
            "\u001b[32m[I 2026-02-11 12:10:45,867]\u001b[0m Trial 9 finished with value: 0.4316070700103781 and parameters: {'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.04754917569113294, 'subsample': 0.7304181585723355, 'colsample_bytree': 0.7291461859424053, 'reg_lambda': 0.012387701742468959, 'reg_alpha': 1.2970527451941831, 'min_child_weight': 243, 'gamma': 1.7270113452917277}. Best is trial 9 with value: 0.4316070700103781.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.04754917569113294, 'subsample': 0.7304181585723355, 'colsample_bytree': 0.7291461859424053, 'reg_lambda': 0.012387701742468959, 'reg_alpha': 1.2970527451941831, 'min_child_weight': 243, 'gamma': 1.7270113452917277}\n"
          ]
        }
      ],
      "source": [
        "def save_model(model, model_path_name):\n",
        "    model_path = Path(model_path_name)\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"Model saved to {model_path.absolute()}\")\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size=0.2,\n",
        "    stratify=y_train,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 800),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 3),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.05, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 10.0, log=True),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 10.0, log=True),\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"logloss\",\n",
        "        \"n_jobs\": -1,\n",
        "        \"early_stopping_rounds\": 50,\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 200, 300),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.5, 5.0),\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False,\n",
        "    )\n",
        "    y_proba = model.predict_proba(X_val)[:, 1]\n",
        "    return log_loss(y_val, y_proba)\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ceefc33b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /Users/lemons/Documents/universidad/cs/pj09-sports-betting/ml/xgboost.joblib\n"
          ]
        }
      ],
      "source": [
        "bst = XGBClassifier(**study.best_params)\n",
        "bst.fit(X_train, y_train)\n",
        "save_model(bst, 'xgboost.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583f579b",
      "metadata": {},
      "source": [
        "**Why probabilities are so extreme (95% / 5%)**\n",
        "\n",
        "1. **XGBoost (and trees) are overconfident** – They output probabilities that get pushed toward 0 and 1. Good ranking, poor calibration.\n",
        "2. **Your features are very informative** – Score + time left often makes the outcome almost certain, so the model is confident.\n",
        "3. **Fix: calibrate** – Fit a calibrator (e.g. isotonic regression) on the model’s outputs so that when it says 70%, about 70% of those cases actually win. The cell below does this and saves the calibrated model for the backend.\n",
        "\n",
        "**Optional training tweaks** (to get slightly softer raw probabilities before calibration): use **max_depth 2–3 only**, **higher min_child_weight** (e.g. 200–400), and **stronger reg_alpha/reg_lambda**. Calibration is still the main fix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "41adf647",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw proba  (min, max): 0.0 1.0\n",
            "Calibrated (min, max): 0.0 1.0\n",
            "Model saved to /Users/lemons/Documents/universidad/cs/pj09-sports-betting/ml/xgboost_calibrated.joblib\n",
            "Calibrated model saved. Restart backend to use it.\n"
          ]
        }
      ],
      "source": [
        "# Calibrate so probabilities are less extreme (e.g. 65/35 instead of 95/5)\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "calibrated = CalibratedClassifierCV(\n",
        "    XGBClassifier(**study.best_params),\n",
        "    method=\"isotonic\",\n",
        "    cv=5,\n",
        ")\n",
        "calibrated.fit(X_train, y_train)\n",
        "\n",
        "# Compare raw vs calibrated on test set\n",
        "p_raw = bst.predict_proba(X_test)[:, 1]\n",
        "p_cal = calibrated.predict_proba(X_test)[:, 1]\n",
        "print(\"Raw proba  (min, max):\", round(p_raw.min(), 3), round(p_raw.max(), 3))\n",
        "print(\"Calibrated (min, max):\", round(p_cal.min(), 3), round(p_cal.max(), 3))\n",
        "\n",
        "save_model(calibrated, \"xgboost_calibrated.joblib\")\n",
        "print(\"Calibrated model saved. Restart backend to use it.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
